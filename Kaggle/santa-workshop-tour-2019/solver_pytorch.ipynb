{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: constraints; cost function? why nega score?; goal is to achieve comparable performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/65/96/c97c8a0ea8f66de41f452925b521bcfdebef6fffb899dc704fc269d87563/torch-1.3.1-cp36-none-macosx_10_7_x86_64.whl (71.1MB)\n",
      "\u001b[K    100% |████████████████████████████████| 71.1MB 657kB/s ta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: numpy in /Users/songxu/Code/venv/lib/python3.6/site-packages (from torch) (1.16.4)\n",
      "Installing collected packages: torch\n",
      "Successfully installed torch-1.3.1\n"
     ]
    }
   ],
   "source": [
    "# https://www.kaggle.com/pavelvod/pytorch-starter-solution\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import importlib\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "# giftCostsNp (5000, 100): [i,j] means the gift cost when i-th family choses j-th day.\n",
    "row, col = 5000, 100\n",
    "giftCostsNp = np.zeros((row, col))\n",
    "familySizesNp = np.zeros((row, 1))\n",
    "\n",
    "familyDataNp = utils.readDataDf().values\n",
    "\n",
    "for i in range(row):\n",
    "    familySizesNp[i] = familyDataNp[i, -1]\n",
    "    for j in range(col):\n",
    "        giftCostsNp[i,j] = utils.getCurGift(assignDay=j+1, \n",
    "                                          familyPreferencesList=familyDataNp[i, 1:-1].tolist(), \n",
    "                                          familySize=familySizesNp[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, giftCostsNp, familySizesNp):\n",
    "        super().__init__()\n",
    "        self.giftCostsTc = torch.from_numpy(giftCostsNp).type(torch.float32)\n",
    "        self.familySizesTc = torch.from_numpy(familySizesNp).type(torch.float32)\n",
    "        self.assignWeightsTc = torch.nn.Parameter(data=torch.Tensor(5000, 100), requires_grad=True)\n",
    "        self.assignWeightsTc.data.uniform_(0,5) # weight initialization?\n",
    "        \n",
    "    def forward(self):\n",
    "        assignProbTc = F.softmax(self.assignWeightsTc, dim=1)\n",
    "#         np.savetxt(\"assignProbNp.txt\", assignProbTc.data.numpy())\n",
    "#         print(self.giftCostsTc.shape, assignProbTc.shape)\n",
    "        giftCostTotalTc = (self.giftCostsTc * assignProbTc).sum()\n",
    "        # giftCostsTc is 5000,100\n",
    "        # assignProbTc is 5000, 100\n",
    "        \n",
    "        NdTc = assignProbTc.transpose(0,1) @ self.familySizesTc\n",
    "        # NdTc = (assignProbTc.transpose(0,1) @ self.familySizesTc).sum(axis=1)\n",
    "        # assignProbTc: 5000, 100\n",
    "        # assignProbTc.transpose: 100, 5000 (0th row: for day-1, each family assigned how much prob. )\n",
    "        # familySizesTc: 5000, 1 (0th col: each family's size. )\n",
    "        # @ result: (100, 1) (0th col: each day's assigned prob weighted by size)\n",
    "        # after sum: (1,1). WTF?!\n",
    "        \n",
    "        \n",
    "#         print(F.softmax(NdTc).max(), F.softmax(NdTc).argmax())\n",
    "        NdTcCat = torch.zeros((101,1), dtype=torch.float32)\n",
    "        \n",
    "        NdTcCat[:-1,0] = NdTc[:,0]\n",
    "#         print(NdTcCat) \n",
    "        NdTcCat[-1,0] = NdTc[-1,0]\n",
    "#         print(NdTcCat) \n",
    "        \n",
    "#         print(\"exponent: \", 0.5+abs(NdTcCat[:-1]-NdTcCat[1:])/50.)\n",
    "        cleanCostTotalTc = (torch.relu(NdTc-125.)/400. * NdTc**(0.5+abs(NdTcCat[:-1]-NdTcCat[1:])/50.)).sum()\n",
    "        \n",
    "        penaltyCostTotalTc = (torch.relu(125.-NdTc)**2 + torch.relu(NdTc-300.)**2).sum() * 1000000\n",
    "        return giftCostTotalTc, cleanCostTotalTc, penaltyCostTotalTc, NdTc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils' from '/Users/songxu/PycharmProjects/Data_Science/Kaggle/santa-workshop-tour-2019/utils.py'>"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def cost_function():\n",
    "#     return None\n",
    "importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Model(giftCostsNp, familySizesNp)\n",
    "best_score = 10e10\n",
    "best_pos = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gift, clean, penalty: 7069079.5 28672.15 0.0\n",
      "totGift, totClean: 1577899 193291407353.20126\n",
      "1000 tensor(7097751.5000, grad_fn=<AddBackward0>) 193292985252.20126\n",
      "\n",
      "gift, clean, penalty: 4161842.2 27331.234 0.0\n",
      "totGift, totClean: 1086337 38862087.481408335\n",
      "2000 tensor(4189173.5000, grad_fn=<AddBackward0>) 39948424.481408335\n",
      "\n",
      "gift, clean, penalty: 2850495.0 13586.006 2.3283064\n",
      "totGift, totClean: 864645 -226699024.39104867\n",
      "3000 tensor(2864083.2500, grad_fn=<AddBackward0>) -225834379.39104867\n",
      "\n",
      "gift, clean, penalty: 2121356.8 10264.638 0.0\n",
      "totGift, totClean: 695745 35045020.90779152\n",
      "4000 tensor(2131621.5000, grad_fn=<AddBackward0>) 35740765.90779152\n",
      "\n",
      "gift, clean, penalty: 1640678.4 8471.3955 0.0\n",
      "totGift, totClean: 557426 1372510.359038832\n",
      "5000 tensor(1649149.7500, grad_fn=<AddBackward0>) 1929936.359038832\n",
      "\n",
      "gift, clean, penalty: 1294422.0 7264.5493 0.0\n",
      "totGift, totClean: 458571 20270034.4935333\n",
      "6000 tensor(1301686.5000, grad_fn=<AddBackward0>) 20728605.4935333\n",
      "\n",
      "gift, clean, penalty: 1035971.44 6774.786 0.0\n",
      "totGift, totClean: 392762 207272.00570352728\n",
      "7000 tensor(1042746.2500, grad_fn=<AddBackward0>) 600034.0057035272\n",
      "\n",
      "gift, clean, penalty: 839234.6 6098.6353 0.0\n",
      "totGift, totClean: 348496 166758.7814510278\n",
      "8000 tensor(845333.2500, grad_fn=<AddBackward0>) 515254.78145102784\n",
      "\n",
      "gift, clean, penalty: 689978.9 6028.681 0.0\n",
      "totGift, totClean: 308715 62763.621040976\n",
      "9000 tensor(696007.5625, grad_fn=<AddBackward0>) 371478.621040976\n",
      "\n",
      "gift, clean, penalty: 576646.0 5890.4834 0.0\n",
      "totGift, totClean: 285637 12835.812943122215\n",
      "10000 tensor(582536.5000, grad_fn=<AddBackward0>) 298472.81294312223\n",
      "\n",
      "gift, clean, penalty: 490443.7 5579.8423 0.0\n",
      "totGift, totClean: 268711 9188.713852120532\n",
      "11000 tensor(496023.5312, grad_fn=<AddBackward0>) 277899.7138521205\n",
      "\n",
      "gift, clean, penalty: 423043.88 5605.705 0.0\n",
      "totGift, totClean: 249324 5689.738732998135\n",
      "12000 tensor(428649.5938, grad_fn=<AddBackward0>) 255013.73873299814\n",
      "\n",
      "gift, clean, penalty: 369826.22 5438.44 0.0\n",
      "totGift, totClean: 234407 12097.865656511663\n",
      "13000 tensor(375264.6562, grad_fn=<AddBackward0>) 246504.86565651165\n",
      "\n",
      "gift, clean, penalty: 328236.12 5402.2163 40.195942\n",
      "totGift, totClean: 220644 -23484.679374401043\n",
      "14000 tensor(333678.5312, grad_fn=<AddBackward0>) 197159.32062559895\n",
      "\n",
      "gift, clean, penalty: 295228.56 5495.331 0.0\n",
      "totGift, totClean: 211078 -253146.56259506286\n",
      "15000 tensor(300723.9062, grad_fn=<AddBackward0>) -42068.56259506286\n",
      "\n",
      "gift, clean, penalty: 269179.25 5764.9927 0.0\n",
      "totGift, totClean: 198511 -216547.17249572594\n",
      "16000 tensor(274944.2500, grad_fn=<AddBackward0>) -18036.172495725943\n",
      "\n",
      "gift, clean, penalty: 248705.78 5900.751 0.0\n",
      "totGift, totClean: 189973 -704889.9424904838\n",
      "17000 tensor(254606.5312, grad_fn=<AddBackward0>) -514916.94249048375\n",
      "\n",
      "gift, clean, penalty: 232483.64 5767.103 93.88313\n",
      "totGift, totClean: 184520 -421089.80358929315\n",
      "18000 tensor(238344.6406, grad_fn=<AddBackward0>) -236569.80358929315\n",
      "\n",
      "gift, clean, penalty: 218972.19 5942.043 0.0\n",
      "totGift, totClean: 178074 -323814.7409120403\n",
      "19000 tensor(224914.2344, grad_fn=<AddBackward0>) -145740.7409120403\n",
      "\n",
      "gift, clean, penalty: 207888.9 5946.6675 30.849129\n",
      "totGift, totClean: 173618 -460354.3110164445\n",
      "20000 tensor(213866.4219, grad_fn=<AddBackward0>) -286736.3110164445\n",
      "\n",
      "gift, clean, penalty: 198335.86 6240.641 0.0\n",
      "totGift, totClean: 168226 -332490.61133650236\n",
      "21000 tensor(204576.5000, grad_fn=<AddBackward0>) -164264.61133650236\n",
      "\n",
      "gift, clean, penalty: 190100.5 5873.094 0.0028521754\n",
      "totGift, totClean: 163558 -207577.83373351383\n",
      "22000 tensor(195973.5938, grad_fn=<AddBackward0>) -44019.833733513835\n",
      "\n",
      "gift, clean, penalty: 182766.17 5800.527 0.0\n",
      "totGift, totClean: 160003 -180277.65632472612\n",
      "23000 tensor(188566.7031, grad_fn=<AddBackward0>) -20274.656324726122\n",
      "\n",
      "gift, clean, penalty: 176371.64 5862.2314 0.0\n",
      "totGift, totClean: 153375 -292754.84535884496\n",
      "24000 tensor(182233.8750, grad_fn=<AddBackward0>) -139379.84535884496\n",
      "\n",
      "gift, clean, penalty: 170644.88 5720.4917 0.0\n",
      "totGift, totClean: 149628 -609308.2604306281\n",
      "25000 tensor(176365.3594, grad_fn=<AddBackward0>) -459680.2604306281\n",
      "\n",
      "gift, clean, penalty: 165769.62 6017.7354 0.0\n",
      "totGift, totClean: 147782 -764062.6975157239\n",
      "26000 tensor(171787.3594, grad_fn=<AddBackward0>) -616280.6975157239\n",
      "\n",
      "gift, clean, penalty: 161255.6 5989.4536 0.0\n",
      "totGift, totClean: 144207 -764425.5512207763\n",
      "27000 tensor(167245.0469, grad_fn=<AddBackward0>) -620218.5512207763\n",
      "\n",
      "gift, clean, penalty: 157185.61 6087.323 23.102621\n",
      "totGift, totClean: 141048 -764582.6568011503\n",
      "28000 tensor(163296.0469, grad_fn=<AddBackward0>) -623534.6568011503\n",
      "\n",
      "gift, clean, penalty: 153422.31 6064.077 7.712282\n",
      "totGift, totClean: 139089 -834207.669102319\n",
      "29000 tensor(159494.1094, grad_fn=<AddBackward0>) -695118.669102319\n",
      "\n",
      "gift, clean, penalty: 150129.44 5793.374 1.2749805\n",
      "totGift, totClean: 136592 -834421.2023437608\n",
      "30000 tensor(155924.0938, grad_fn=<AddBackward0>) -697829.2023437608\n",
      "\n",
      "gift, clean, penalty: 147145.44 6076.9917 0.0\n",
      "totGift, totClean: 134167 -587583.995885695\n",
      "31000 tensor(153222.4219, grad_fn=<AddBackward0>) -453416.995885695\n",
      "\n",
      "gift, clean, penalty: 144426.66 6193.031 0.00052386895\n",
      "totGift, totClean: 132385 -588384.9936350782\n",
      "32000 tensor(150619.6875, grad_fn=<AddBackward0>) -455999.9936350782\n",
      "\n",
      "gift, clean, penalty: 141871.81 6148.2876 32.56742\n",
      "totGift, totClean: 131039 -450805.1935860112\n",
      "33000 tensor(148052.6562, grad_fn=<AddBackward0>) -319766.1935860112\n",
      "\n",
      "gift, clean, penalty: 139510.62 5864.41 0.0\n",
      "totGift, totClean: 129449 -450955.1010963104\n",
      "34000 tensor(145375.0312, grad_fn=<AddBackward0>) -321506.1010963104\n",
      "\n",
      "gift, clean, penalty: 137322.94 6232.207 0.29342481\n",
      "totGift, totClean: 127575 -360141.28235886636\n",
      "35000 tensor(143555.4375, grad_fn=<AddBackward0>) -232566.28235886636\n",
      "\n",
      "gift, clean, penalty: 135341.6 6113.5083 0.0\n",
      "totGift, totClean: 126338 -359039.83607030776\n",
      "36000 tensor(141455.1094, grad_fn=<AddBackward0>) -232701.83607030776\n",
      "\n",
      "gift, clean, penalty: 133446.08 5755.765 1.206994\n",
      "totGift, totClean: 124715 -277951.1756121311\n",
      "37000 tensor(139203.0469, grad_fn=<AddBackward0>) -153236.17561213108\n",
      "\n",
      "gift, clean, penalty: 131571.19 6089.304 0.0\n",
      "totGift, totClean: 123201 -440274.27542948385\n",
      "38000 tensor(137660.4844, grad_fn=<AddBackward0>) -317073.27542948385\n",
      "\n",
      "gift, clean, penalty: 129786.84 5983.28 6.121816\n",
      "totGift, totClean: 122053 -438945.0668085535\n",
      "39000 tensor(135776.2500, grad_fn=<AddBackward0>) -316892.0668085535\n",
      "\n",
      "gift, clean, penalty: 128096.414 6001.875 0.0\n",
      "totGift, totClean: 121069 -440164.2910857307\n",
      "40000 tensor(134098.2812, grad_fn=<AddBackward0>) -319095.2910857307\n",
      "\n",
      "gift, clean, penalty: 126518.85 5730.1284 3.2694662\n",
      "totGift, totClean: 120466 -302567.0944780175\n",
      "41000 tensor(132252.2500, grad_fn=<AddBackward0>) -182101.09447801748\n",
      "\n",
      "gift, clean, penalty: 125044.46 5822.6143 0.0\n",
      "totGift, totClean: 118825 -263747.18507693603\n",
      "42000 tensor(130867.0781, grad_fn=<AddBackward0>) -144922.18507693603\n",
      "\n",
      "gift, clean, penalty: 123635.07 5756.063 113.84976\n",
      "totGift, totClean: 117941 -263964.4021274744\n",
      "43000 tensor(129504.9844, grad_fn=<AddBackward0>) -146023.40212747437\n",
      "\n",
      "gift, clean, penalty: 122315.625 6026.5796 0.0\n",
      "totGift, totClean: 116682 -385759.98280346463\n",
      "44000 tensor(128342.2031, grad_fn=<AddBackward0>) -269077.98280346463\n",
      "\n",
      "gift, clean, penalty: 121010.17 6095.945 0.0\n",
      "totGift, totClean: 115797 -387610.1670923192\n",
      "45000 tensor(127106.1172, grad_fn=<AddBackward0>) -271813.1670923192\n",
      "\n",
      "gift, clean, penalty: 119818.77 5990.541 22.133171\n",
      "totGift, totClean: 113791 -390661.4153632139\n",
      "46000 tensor(125831.4453, grad_fn=<AddBackward0>) -276870.4153632139\n",
      "\n",
      "gift, clean, penalty: 118696.19 5833.9443 0.43050385\n",
      "totGift, totClean: 113102 -389395.92172164604\n",
      "47000 tensor(124530.5625, grad_fn=<AddBackward0>) -276293.92172164604\n",
      "\n",
      "gift, clean, penalty: 117606.33 6138.755 0.28521752\n",
      "totGift, totClean: 112599 -389805.2219729232\n",
      "48000 tensor(123745.3750, grad_fn=<AddBackward0>) -277206.2219729232\n",
      "\n",
      "gift, clean, penalty: 116538.055 6082.8774 0.0\n",
      "totGift, totClean: 111634 -387455.33017344674\n",
      "49000 tensor(122620.9297, grad_fn=<AddBackward0>) -275821.33017344674\n",
      "\n",
      "gift, clean, penalty: 115559.87 6344.7314 0.0\n",
      "totGift, totClean: 111185 -388514.7216610779\n",
      "50000 tensor(121904.6016, grad_fn=<AddBackward0>) -277329.7216610779\n",
      "\n",
      "gift, clean, penalty: 114635.05 6291.2666 0.0\n",
      "totGift, totClean: 110083 -389403.70344471344\n",
      "51000 tensor(120926.3125, grad_fn=<AddBackward0>) -279320.70344471344\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gift, clean, penalty: 113715.31 5802.137 29.095041\n",
      "totGift, totClean: 108782 -389136.29090449173\n",
      "52000 tensor(119546.5469, grad_fn=<AddBackward0>) -280354.29090449173\n",
      "\n",
      "gift, clean, penalty: 112866.68 5726.0825 14.320016\n",
      "totGift, totClean: 108387 -392710.06457993167\n",
      "53000 tensor(118607.0859, grad_fn=<AddBackward0>) -284323.06457993167\n",
      "\n",
      "gift, clean, penalty: 112058.63 5814.0093 0.0\n",
      "totGift, totClean: 106773 -392561.27488662204\n",
      "54000 tensor(117872.6406, grad_fn=<AddBackward0>) -285788.27488662204\n",
      "\n",
      "gift, clean, penalty: 111257.836 5821.703 0.0\n",
      "totGift, totClean: 106229 -393440.9738434293\n",
      "55000 tensor(117079.5391, grad_fn=<AddBackward0>) -287211.9738434293\n",
      "\n",
      "gift, clean, penalty: 110478.43 6294.6284 0.0\n",
      "totGift, totClean: 105758 -390466.3971816985\n",
      "56000 tensor(116773.0547, grad_fn=<AddBackward0>) -284708.3971816985\n",
      "\n",
      "gift, clean, penalty: 109720.055 5799.4956 0.0\n",
      "totGift, totClean: 104838 -393974.0193994652\n",
      "57000 tensor(115519.5469, grad_fn=<AddBackward0>) -289136.0193994652\n",
      "\n",
      "gift, clean, penalty: 108966.86 5730.7495 0.0\n",
      "totGift, totClean: 104552 -399900.5112693311\n",
      "58000 tensor(114697.6094, grad_fn=<AddBackward0>) -295348.5112693311\n",
      "\n",
      "gift, clean, penalty: 108239.445 5782.1367 13.411045\n",
      "totGift, totClean: 103387 -392071.3134623155\n",
      "59000 tensor(114034.9922, grad_fn=<AddBackward0>) -288684.3134623155\n",
      "\n",
      "gift, clean, penalty: 107520.23 5884.621 0.0\n",
      "totGift, totClean: 102390 -391876.76334179915\n",
      "60000 tensor(113404.8438, grad_fn=<AddBackward0>) -289486.76334179915\n",
      "\n",
      "gift, clean, penalty: 106818.83 6018.334 0.895001\n",
      "totGift, totClean: 102213 -392031.8064229581\n",
      "61000 tensor(112838.0625, grad_fn=<AddBackward0>) -289818.8064229581\n",
      "\n",
      "gift, clean, penalty: 106150.734 5927.8394 1.974986\n",
      "totGift, totClean: 102322 -392448.8839616207\n",
      "62000 tensor(112080.5469, grad_fn=<AddBackward0>) -290126.8839616207\n",
      "\n",
      "gift, clean, penalty: 105488.695 5879.2124 0.0\n",
      "totGift, totClean: 101298 -392403.3474253802\n",
      "63000 tensor(111367.9062, grad_fn=<AddBackward0>) -291105.3474253802\n",
      "\n",
      "gift, clean, penalty: 104819.984 6022.8896 0.0\n",
      "totGift, totClean: 101148 -270645.2718702586\n",
      "64000 tensor(110842.8750, grad_fn=<AddBackward0>) -169497.2718702586\n",
      "\n",
      "gift, clean, penalty: 104214.24 6020.4385 7.8399315\n",
      "totGift, totClean: 100286 -429746.41062802944\n",
      "65000 tensor(110242.5234, grad_fn=<AddBackward0>) -329460.41062802944\n",
      "\n",
      "gift, clean, penalty: 103606.805 5757.1577 102.19083\n",
      "totGift, totClean: 99773 -323582.8001675464\n",
      "66000 tensor(109466.1484, grad_fn=<AddBackward0>) -223809.80016754637\n",
      "\n",
      "gift, clean, penalty: 103092.47 5636.929 81.09805\n",
      "totGift, totClean: 98785 -201054.2201878411\n",
      "67000 tensor(108810.5000, grad_fn=<AddBackward0>) -102269.2201878411\n",
      "\n",
      "gift, clean, penalty: 102573.14 5925.7026 5.486014\n",
      "totGift, totClean: 98246 -201575.6985813826\n",
      "68000 tensor(108504.3281, grad_fn=<AddBackward0>) -103329.6985813826\n",
      "\n",
      "gift, clean, penalty: 102072.22 5987.9277 0.07130438\n",
      "totGift, totClean: 98364 -289512.22903915454\n",
      "69000 tensor(108060.2188, grad_fn=<AddBackward0>) -191148.22903915454\n",
      "\n",
      "gift, clean, penalty: 101625.89 6006.8306 0.41071326\n",
      "totGift, totClean: 98155 -459162.3386456459\n",
      "70000 tensor(107633.1328, grad_fn=<AddBackward0>) -361007.3386456459\n",
      "\n",
      "gift, clean, penalty: 101169.16 5751.5435 0.028172508\n",
      "totGift, totClean: 97656 -460619.1792124545\n",
      "71000 tensor(106920.7344, grad_fn=<AddBackward0>) -362963.1792124545\n",
      "\n",
      "gift, clean, penalty: 100744.9 5743.2515 0.0\n",
      "totGift, totClean: 97434 -218258.6786907547\n",
      "72000 tensor(106488.1484, grad_fn=<AddBackward0>) -120824.67869075469\n",
      "\n",
      "gift, clean, penalty: 100350.41 5854.6133 0.0\n",
      "totGift, totClean: 97171 -218288.24034200463\n",
      "73000 tensor(106205.0156, grad_fn=<AddBackward0>) -121117.24034200463\n",
      "\n",
      "gift, clean, penalty: 99950.805 5862.291 18.515625\n",
      "totGift, totClean: 96976 -214215.2020380799\n",
      "74000 tensor(105831.6094, grad_fn=<AddBackward0>) -117239.20203807991\n",
      "\n",
      "gift, clean, penalty: 99547.18 5709.8413 148.80847\n",
      "totGift, totClean: 97026 -347779.2324128839\n",
      "75000 tensor(105405.8281, grad_fn=<AddBackward0>) -250753.2324128839\n",
      "\n",
      "gift, clean, penalty: 99193.336 5975.5825 25.541115\n",
      "totGift, totClean: 96577 -298560.53776597953\n",
      "76000 tensor(105194.4609, grad_fn=<AddBackward0>) -201983.53776597953\n",
      "\n",
      "gift, clean, penalty: 98847.21 6187.385 0.0\n",
      "totGift, totClean: 96649 -294034.99943632336\n",
      "77000 tensor(105034.5938, grad_fn=<AddBackward0>) -197385.99943632336\n",
      "\n",
      "gift, clean, penalty: 98507.24 5869.8228 34.194965\n",
      "totGift, totClean: 96631 -294558.31393725326\n",
      "78000 tensor(104411.2578, grad_fn=<AddBackward0>) -197927.31393725326\n",
      "\n",
      "gift, clean, penalty: 98180.61 5902.4834 49.80975\n",
      "totGift, totClean: 96336 -299982.29544200836\n",
      "79000 tensor(104132.9062, grad_fn=<AddBackward0>) -203646.29544200836\n",
      "\n",
      "gift, clean, penalty: 97881.516 6168.649 0.0\n",
      "totGift, totClean: 95769 -176830.47088099926\n",
      "80000 tensor(104050.1641, grad_fn=<AddBackward0>) -81061.47088099926\n",
      "\n",
      "gift, clean, penalty: 97570.24 5844.506 0.0\n",
      "totGift, totClean: 95538 -292227.13212334475\n",
      "81000 tensor(103414.7500, grad_fn=<AddBackward0>) -196689.13212334475\n",
      "\n",
      "gift, clean, penalty: 97278.35 5964.4663 0.028172508\n",
      "totGift, totClean: 95067 -177650.16250705265\n",
      "82000 tensor(103242.8516, grad_fn=<AddBackward0>) -82583.16250705265\n",
      "\n",
      "gift, clean, penalty: 96979.016 6148.796 8.771429\n",
      "totGift, totClean: 94495 -289382.22265782\n",
      "83000 tensor(103136.5859, grad_fn=<AddBackward0>) -194887.22265781998\n",
      "\n",
      "gift, clean, penalty: 96692.305 5757.11 0.0009313226\n",
      "totGift, totClean: 94377 -289571.7784460282\n",
      "84000 tensor(102449.4141, grad_fn=<AddBackward0>) -195194.77844602818\n",
      "\n",
      "gift, clean, penalty: 96419.15 5820.7656 0.0\n",
      "totGift, totClean: 94432 -139897.34482627086\n",
      "85000 tensor(102239.9141, grad_fn=<AddBackward0>) -45465.344826270855\n",
      "\n",
      "gift, clean, penalty: 96146.16 5694.1274 30.160881\n",
      "totGift, totClean: 93584 -139987.1970256725\n",
      "86000 tensor(101870.4453, grad_fn=<AddBackward0>) -46403.1970256725\n",
      "\n",
      "gift, clean, penalty: 95863.83 5846.1816 0.0\n",
      "totGift, totClean: 93525 -134375.78639649265\n",
      "87000 tensor(101710.0078, grad_fn=<AddBackward0>) -40850.78639649265\n",
      "\n",
      "gift, clean, penalty: 95596.51 5706.8745 21.467218\n",
      "totGift, totClean: 93430 -134158.19399154987\n",
      "88000 tensor(101324.8516, grad_fn=<AddBackward0>) -40728.193991549866\n",
      "\n",
      "gift, clean, penalty: 95327.54 6002.507 3.3458927\n",
      "totGift, totClean: 93053 -312742.37564101216\n",
      "89000 tensor(101333.3906, grad_fn=<AddBackward0>) -219689.37564101216\n",
      "\n",
      "gift, clean, penalty: 95087.36 5769.3413 1.6039703\n",
      "totGift, totClean: 93203 -317891.3622214892\n",
      "90000 tensor(100858.3047, grad_fn=<AddBackward0>) -224688.3622214892\n",
      "\n",
      "gift, clean, penalty: 94832.05 5844.6367 10.417891\n",
      "totGift, totClean: 93031 -317425.4484170612\n",
      "91000 tensor(100687.1016, grad_fn=<AddBackward0>) -224394.44841706118\n",
      "\n",
      "gift, clean, penalty: 94576.12 6073.944 16.350533\n",
      "totGift, totClean: 92700 -317037.1575600123\n",
      "92000 tensor(100666.4141, grad_fn=<AddBackward0>) -224337.1575600123\n",
      "\n",
      "gift, clean, penalty: 94317.88 5830.854 57.85894\n",
      "totGift, totClean: 92528 -316978.6681213208\n",
      "93000 tensor(100206.5938, grad_fn=<AddBackward0>) -224450.66812132078\n",
      "\n",
      "gift, clean, penalty: 94080.06 5617.0337 4.5634804\n",
      "totGift, totClean: 92324 -190869.82320045802\n",
      "94000 tensor(99701.6562, grad_fn=<AddBackward0>) -98545.82320045802\n",
      "\n",
      "gift, clean, penalty: 93838.07 5846.8022 0.0\n",
      "totGift, totClean: 91925 -188570.37389884738\n",
      "95000 tensor(99684.8750, grad_fn=<AddBackward0>) -96645.37389884738\n",
      "\n",
      "gift, clean, penalty: 93617.41 5878.789 69.66502\n",
      "totGift, totClean: 91893 -189302.80654253176\n",
      "96000 tensor(99565.8594, grad_fn=<AddBackward0>) -97409.80654253176\n",
      "\n",
      "gift, clean, penalty: 93382.63 5723.4907 37.90768\n",
      "totGift, totClean: 91825 -190390.72502827615\n",
      "97000 tensor(99144.0312, grad_fn=<AddBackward0>) -98565.72502827615\n",
      "\n",
      "gift, clean, penalty: 93151.05 5703.7646 6.588234\n",
      "totGift, totClean: 91694 -190730.7880194436\n",
      "98000 tensor(98861.3984, grad_fn=<AddBackward0>) -99036.7880194436\n",
      "\n",
      "gift, clean, penalty: 92909.84 5672.721 2.7756906\n",
      "totGift, totClean: 91594 -189445.99485836312\n",
      "99000 tensor(98585.3359, grad_fn=<AddBackward0>) -97851.99485836312\n",
      "\n",
      "gift, clean, penalty: 92671.04 6182.9937 0.0\n",
      "totGift, totClean: 91698 -196981.85278327134\n",
      "100000 tensor(98854.0312, grad_fn=<AddBackward0>) -105283.85278327134\n",
      "\n",
      "gift, clean, penalty: 92458.24 5977.515 0.0\n",
      "totGift, totClean: 91884 -197699.13603910836\n",
      "101000 tensor(98435.7578, grad_fn=<AddBackward0>) -105815.13603910836\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gift, clean, penalty: 92238.61 5708.4316 361.18326\n",
      "totGift, totClean: 91630 -197780.6508616921\n",
      "102000 tensor(98308.2188, grad_fn=<AddBackward0>) -106150.65086169209\n",
      "\n",
      "gift, clean, penalty: 92027.39 5704.4585 0.013096724\n",
      "totGift, totClean: 91680 -323597.6243644643\n",
      "103000 tensor(97731.8672, grad_fn=<AddBackward0>) -231917.6243644643\n",
      "\n",
      "gift, clean, penalty: 91835.34 6036.6797 0.15139812\n",
      "totGift, totClean: 91186 -197682.45445398934\n",
      "104000 tensor(97872.1719, grad_fn=<AddBackward0>) -106496.45445398934\n",
      "\n",
      "gift, clean, penalty: 91651.08 5611.404 1.1736993\n",
      "totGift, totClean: 91023 -197605.77641693145\n",
      "105000 tensor(97263.6562, grad_fn=<AddBackward0>) -106582.77641693145\n",
      "\n",
      "gift, clean, penalty: 91417.96 6182.1045 66.64195\n",
      "totGift, totClean: 90750 -235876.47208881986\n",
      "106000 tensor(97666.7031, grad_fn=<AddBackward0>) -145126.47208881986\n",
      "\n",
      "gift, clean, penalty: 91228.78 5621.3647 0.0\n",
      "totGift, totClean: 90248 -180887.21524026\n",
      "107000 tensor(96850.1484, grad_fn=<AddBackward0>) -90639.21524026\n",
      "\n",
      "gift, clean, penalty: 91033.71 5786.46 0.6260816\n",
      "totGift, totClean: 90035 -121800.868064471\n",
      "108000 tensor(96820.7969, grad_fn=<AddBackward0>) -31765.868064470997\n",
      "\n",
      "gift, clean, penalty: 90830.016 6282.012 4.7611537\n",
      "totGift, totClean: 89831 -121628.86413035494\n",
      "109000 tensor(97116.7891, grad_fn=<AddBackward0>) -31797.864130354938\n",
      "\n",
      "gift, clean, penalty: 90649.12 5942.5625 0.020547304\n",
      "totGift, totClean: 89487 -122444.42777680553\n",
      "110000 tensor(96591.7031, grad_fn=<AddBackward0>) -32957.42777680553\n",
      "\n",
      "gift, clean, penalty: 90483.69 5888.669 0.895001\n",
      "totGift, totClean: 89514 -124145.71503520149\n",
      "111000 tensor(96373.2578, grad_fn=<AddBackward0>) -34631.71503520149\n",
      "\n",
      "gift, clean, penalty: 90304.586 5869.9272 18.790888\n",
      "totGift, totClean: 89519 -118735.85869050112\n",
      "112000 tensor(96193.3047, grad_fn=<AddBackward0>) -29216.858690501118\n",
      "\n",
      "gift, clean, penalty: 90124.72 5956.571 23.39692\n",
      "totGift, totClean: 89288 -118292.05018557464\n",
      "113000 tensor(96104.6875, grad_fn=<AddBackward0>) -29004.05018557464\n",
      "\n",
      "gift, clean, penalty: 89964.96 5745.6025 0.0\n",
      "totGift, totClean: 88889 -114996.66286508444\n",
      "114000 tensor(95710.5625, grad_fn=<AddBackward0>) -26107.662865084436\n",
      "\n",
      "gift, clean, penalty: 89782.8 6245.1377 0.0\n",
      "totGift, totClean: 88939 -115072.09278224852\n",
      "115000 tensor(96027.9375, grad_fn=<AddBackward0>) -26133.092782248525\n",
      "\n",
      "gift, clean, penalty: 89635.984 6112.8154 8.232484\n",
      "totGift, totClean: 88821 -114115.54907808769\n",
      "116000 tensor(95757.0312, grad_fn=<AddBackward0>) -25294.549078087686\n",
      "\n",
      "gift, clean, penalty: 89460.47 6111.927 6.6105857\n",
      "totGift, totClean: 88821 -114115.54907808769\n",
      "117000 tensor(95579.0078, grad_fn=<AddBackward0>) -25294.549078087686\n",
      "\n",
      "gift, clean, penalty: 89298.57 6009.7656 0.0\n",
      "totGift, totClean: 88717 -113877.43397565991\n",
      "118000 tensor(95308.3359, grad_fn=<AddBackward0>) -25160.433975659907\n",
      "\n",
      "gift, clean, penalty: 89126.09 6292.76 3.8146973\n",
      "totGift, totClean: 88033 -114969.75530283127\n",
      "119000 tensor(95422.6641, grad_fn=<AddBackward0>) -26936.75530283127\n",
      "\n",
      "gift, clean, penalty: 88979.836 5743.9673 0.0\n",
      "totGift, totClean: 87453 -100116.63228747013\n",
      "120000 tensor(94723.8047, grad_fn=<AddBackward0>) -12663.632287470129\n",
      "\n",
      "gift, clean, penalty: 88831.016 6166.2817 0.0\n",
      "totGift, totClean: 87403 -100121.3609472612\n",
      "121000 tensor(94997.2969, grad_fn=<AddBackward0>) -12718.3609472612\n",
      "\n",
      "gift, clean, penalty: 88682.73 5752.6787 161.85414\n",
      "totGift, totClean: 87050 -110454.5950937087\n",
      "122000 tensor(94597.2578, grad_fn=<AddBackward0>) -23404.595093708704\n",
      "\n",
      "gift, clean, penalty: 88523.14 5787.8223 2.4471083\n",
      "totGift, totClean: 86864 -109612.68694386265\n",
      "123000 tensor(94313.4062, grad_fn=<AddBackward0>) -22748.686943862645\n",
      "\n",
      "gift, clean, penalty: 88382.016 6098.1934 0.014901161\n",
      "totGift, totClean: 86927 -109999.26015790766\n",
      "124000 tensor(94480.2266, grad_fn=<AddBackward0>) -23072.260157907658\n",
      "\n",
      "gift, clean, penalty: 88239.3 6012.0737 13.861805\n",
      "totGift, totClean: 86728 -107642.79788208213\n",
      "125000 tensor(94265.2266, grad_fn=<AddBackward0>) -20914.79788208213\n",
      "\n",
      "gift, clean, penalty: 88106.21 5800.9116 0.0\n",
      "totGift, totClean: 86873 -107842.40187551822\n",
      "126000 tensor(93907.1250, grad_fn=<AddBackward0>) -20969.40187551822\n",
      "\n",
      "gift, clean, penalty: 87964.98 5691.8374 5.8861914\n",
      "totGift, totClean: 86774 -107972.55550080427\n",
      "127000 tensor(93662.6953, grad_fn=<AddBackward0>) -21198.555500804272\n",
      "\n",
      "gift, clean, penalty: 87820.13 6082.5923 33.97558\n",
      "totGift, totClean: 86330 -105724.61356399645\n",
      "128000 tensor(93936.7031, grad_fn=<AddBackward0>) -19394.61356399645\n",
      "\n",
      "gift, clean, penalty: 87676.195 5927.6167 7.0491805\n",
      "totGift, totClean: 86325 -100693.21367843504\n",
      "129000 tensor(93610.8594, grad_fn=<AddBackward0>) -14368.213678435044\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-436-7ba46f1014f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#     assignProbNp = F.softmax(model.assignWeightsTc, dim=1).data.numpy()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Code/venv/lib/python3.6/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model1.parameters())\n",
    "for epoch in range(1,150000):\n",
    "    optimizer.zero_grad()\n",
    "    x, y, z, NdTc = model1()\n",
    "#     print(x.shape, y.shape)\n",
    "    loss = x + y + z\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "#     assignProbNp = F.softmax(model.assignWeightsTc, dim=1).data.numpy()    \n",
    "#     np.savetxt(\"intermediate/assignProbNp.txt\", assignProbNp)\n",
    "    \n",
    "#     break\n",
    "    \n",
    "    \n",
    "    if epoch % 1000 == 0:\n",
    "        \n",
    "        # (5000, 100): for each family (keep dim 0), find the most-weighted day\n",
    "        \n",
    "#         print(model.assignWeightsTc[0,:])\n",
    "#         print(model.assignWeightsTc[0,:].max(), model.assignWeightsTc[0,:].argmax())\n",
    "        \n",
    "#         print(assignments.max(), assignments.min())\n",
    "\n",
    "        assignments = model1.assignWeightsTc.argmax(dim=1).numpy()+1\n",
    "#         assignProbNp = F.softmax(model.assignWeightsTc, dim=1).data.numpy()    \n",
    "#         np.savetxt(\"intermediate/assignProbNp.txt\", assignProbNp)\n",
    "\n",
    "#         NdTensorNp = NdTc.data.numpy()\n",
    "#         np.savetxt(\"intermediate/NdTensorNp.txt\", NdTensorNp)\n",
    "#         with open(, \"w\") as fOut:\n",
    "#             rowOut, colOut = assignProbNp.shape\n",
    "#             for i in range(rowOut):\n",
    "#                 fOut.write(assignProbNp[i, :])\n",
    "#                 fOut.write(\"\\n\")\n",
    "        print(\"gift, clean, penalty:\", x.data.numpy(),y.data.numpy(),z.data.numpy())\n",
    "\n",
    "        curScore = utils.getTotalScore(assignments, verbose=True)\n",
    "        \n",
    "        print(epoch, loss, curScore)\n",
    "        print()\n",
    "    \n",
    "#         print(\"Here!\", epoch, \"loss:\", loss, \"score:\", curScore)\n",
    "    \n",
    "#         print(\"NdTc[:10]:\", NdTc[:10].data)\n",
    "    \n",
    "#         print(x, y, z)\n",
    "#         print(curScore, z, loss)\n",
    "#         break\n",
    "#         pos = model.assignWeightsTc.argmax(1).numpy()\n",
    "#         a, b, v = cost_function(pos+1)\n",
    "#         score = a + b\n",
    "#         if score < best_score:\n",
    "#             best_score = score\n",
    "#             best_pos = pos\n",
    "#         x = np.round(x.item(), 3)\n",
    "#         y = np.round(y.item(), 3)\n",
    "#         print(f\"{epoch}\\t{x}\\t{y}\\t{score}\\t{a}\\t{b}\\t{v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils' from '/Users/songxu/PycharmProjects/Data_Science/Kaggle/santa-workshop-tour-2019/utils.py'>"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([53, 27, 55, ..., 33, 93, 14])"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[99, 104, 242, 269, 300, 270, 249, 235, 258, 265, 301, 308, 301, 271, 256, 248, 275, 287, 282, 276, 242, 212, 220, 262, 283, 297, 305, 273, 255, 244, 244, 289, 271, 261, 227, 194, 176, 205, 224, 248, 235, 203, 174, 169, 203, 243, 269, 236, 216, 204, 179, 218, 246, 239, 211, 190, 151, 164, 203, 234, 230, 199, 165, 113, 122, 129, 242, 224, 186, 143, 127, 123, 110, 227, 203, 170, 130, 117, 132, 133, 219, 207, 177, 131, 124, 125, 131, 249, 229, 201, 157, 127, 130, 131, 314, 261, 215, 177, 132, 126]\n",
      "totGift, totClean: 10588772 9452870.457375018\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20041642.45737502"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.getTotalScore(assignments, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[125.63609313964844],\n",
       " [225.6883087158203],\n",
       " [262.4656982421875],\n",
       " [287.7615966796875],\n",
       " [281.980712890625],\n",
       " [259.5467224121094],\n",
       " [251.5950469970703],\n",
       " [249.47315979003906],\n",
       " [267.9544677734375],\n",
       " [292.07012939453125],\n",
       " [299.1529541015625],\n",
       " [299.94976806640625],\n",
       " [279.8957214355469],\n",
       " [267.5335998535156],\n",
       " [261.9139404296875],\n",
       " [279.3463134765625],\n",
       " [299.634521484375],\n",
       " [298.6199035644531],\n",
       " [282.5728759765625],\n",
       " [256.8014831542969],\n",
       " [233.29623413085938],\n",
       " [238.39437866210938],\n",
       " [261.3100891113281],\n",
       " [286.94708251953125],\n",
       " [299.59796142578125],\n",
       " [299.19329833984375],\n",
       " [284.35491943359375],\n",
       " [269.676025390625],\n",
       " [254.2886505126953],\n",
       " [248.31134033203125],\n",
       " [275.40814208984375],\n",
       " [279.37115478515625],\n",
       " [259.62005615234375],\n",
       " [228.44033813476562],\n",
       " [201.4298095703125],\n",
       " [178.33648681640625],\n",
       " [202.716064453125],\n",
       " [236.244384765625],\n",
       " [259.9376220703125],\n",
       " [241.8492889404297],\n",
       " [211.04945373535156],\n",
       " [181.42010498046875],\n",
       " [183.7914581298828],\n",
       " [211.67327880859375],\n",
       " [247.53042602539062],\n",
       " [267.4666748046875],\n",
       " [252.0206756591797],\n",
       " [225.17921447753906],\n",
       " [203.80429077148438],\n",
       " [186.78631591796875],\n",
       " [218.33895874023438],\n",
       " [249.99624633789062],\n",
       " [248.9009246826172],\n",
       " [227.0052490234375],\n",
       " [193.94818115234375],\n",
       " [168.0062255859375],\n",
       " [156.4180908203125],\n",
       " [200.35491943359375],\n",
       " [234.76370239257812],\n",
       " [231.76043701171875],\n",
       " [201.18991088867188],\n",
       " [161.2587890625],\n",
       " [125.20307922363281],\n",
       " [125.2110824584961],\n",
       " [125.05201721191406],\n",
       " [235.27975463867188],\n",
       " [209.18350219726562],\n",
       " [172.55606079101562],\n",
       " [125.69942474365234],\n",
       " [125.14537811279297],\n",
       " [125.13155364990234],\n",
       " [125.34515380859375],\n",
       " [216.06259155273438],\n",
       " [202.73854064941406],\n",
       " [171.95513916015625],\n",
       " [125.21482849121094],\n",
       " [127.16435241699219],\n",
       " [125.2940673828125],\n",
       " [125.06582641601562],\n",
       " [218.26283264160156],\n",
       " [207.71517944335938],\n",
       " [175.15853881835938],\n",
       " [130.14952087402344],\n",
       " [125.24139404296875],\n",
       " [125.03882598876953],\n",
       " [125.08294677734375],\n",
       " [225.85337829589844],\n",
       " [228.5526123046875],\n",
       " [198.70162963867188],\n",
       " [158.03292846679688],\n",
       " [125.05178833007812],\n",
       " [125.20355224609375],\n",
       " [125.03931427001953],\n",
       " [217.12918090820312],\n",
       " [203.06231689453125],\n",
       " [171.26158142089844],\n",
       " [125.02471923828125],\n",
       " [125.030517578125],\n",
       " [125.11773681640625],\n",
       " [125.00289916992188]]"
      ]
     },
     "execution_count": 431,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NdTc.data.numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_100000run = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
